{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d341a948",
   "metadata": {},
   "source": [
    "Nozomu Nakanishi - 2020264 - Msc DA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e17543f",
   "metadata": {},
   "source": [
    "This notebook presents the technical part of the research which aims to develop a neural network for sentiment analysis using LSTM.\n",
    "The chronological order will follow: \n",
    "- Introduction;\n",
    "- Data preparation and EDA; \n",
    "- Text pre-processing;\n",
    "- Padding and Word2Vec;\n",
    "- Notes; \n",
    "- LSTM; "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0692e",
   "metadata": {},
   "source": [
    "## 1. Introduction \n",
    "\n",
    "The experiment will be carried out by importing the chosen dataset from Kaggle, the data is licensed under the Creative Commons (CC0 1.0 DEED), allowing the use of the data. \n",
    "\n",
    "The link for the dataset is available at kaggle: https://www.kaggle.com/datasets/irkaal/foodcom-recipes-and-reviews\n",
    "\n",
    "The objective of the notebook is to develop a neural network using PySpark (chosen tool for the project) and the chosen dataset to achieve the goal is the recipe reviews from food.com, which is  composed by 8 features and more tham a million rows.\n",
    "- There are two datasets in the zip file, however it does not says that the recipe ID in one will match the recipe ID in the other, therefore using only the reviews dataset.\n",
    "- Also, two types of format is available, parquet and csv files. And the choice for the first one is that the parquet file already has a columnar type, which when taking into consideration the tools that are being used such as pyspark it will present a better speed perfomance when performing tasks. In addition to the reduction for its size in megabytes (csv - 496.1 and 173.8). \n",
    "- The dataset review, which is going to be manipulated, is about the written reviews along with score rating given. They are the most important features in the dataset to achieve the objective. \n",
    "\n",
    "The practical part of the task will be carried out by performing sentiment analysis using Long Short Term Memory (LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6de0a9d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The command below is to check where the PySpark is running, \n",
    "# in this case (returned:local[*]) data is being processed on my local (personal) machine.\n",
    "\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624cee5c",
   "metadata": {},
   "source": [
    "- The libraries to perform the required stages are going to be imported in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09e85221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 19:52:40.448974: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-11 19:52:42.198588: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-10-11 19:52:42.198654: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-10-11 19:52:42.553200: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-10-11 19:52:45.557730: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-11 19:52:45.558487: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-10-11 19:52:45.558521: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count # Funcion to get the \"size\" of the data.\n",
    "from pyspark.sql.functions import when # When function.\n",
    "from pyspark.sql.functions import col # Function column.\n",
    "from pyspark.sql.functions import mean, min, max, stddev # Imports function for statistical features. \n",
    "from pyspark.sql import functions as F # Data processing framework.\n",
    "from pyspark.sql.functions import lower # lower case function.\n",
    "from pyspark.sql.functions import length # Import lenght function.\n",
    "from pyspark.sql.functions import size, split # Imports function size and split.\n",
    "from pyspark.ml.feature import Tokenizer # Importing Tokenizer.\n",
    "from pyspark.sql.functions import regexp_replace # Remove / Replace function.\n",
    "from pyspark.sql.functions import explode, desc # Importing for word count.\n",
    "from pyspark.ml.feature import StopWordsRemover # Imports stopword remover.\n",
    "from pyspark.sql.types import StructField, StructType # Importing features for Schema.\n",
    "from pyspark.sql.types import IntegerType, StringType, TimestampType # Tools to create the schema.\n",
    "from pyspark.sql.types import FloatType # FloatType (data)\n",
    "from pyspark.sql.functions import udf # Imports function UDF (user defined functions).\n",
    "from pyspark.sql.functions import pandas_udf # user defined functions\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder # enconde labels\n",
    "from pyspark.ml import Pipeline # Creates a pipeline for the encoder\n",
    "from pyspark.sql.types import ArrayType # Array types of data\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import max as max_\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import spacy # Lemmatization tool\n",
    "import numpy as np # for numerical operations.\n",
    "import seaborn as sns # Visualization tool.\n",
    "import matplotlib.pyplot as plt # visualization\n",
    "%matplotlib inline \n",
    "\n",
    "import warnings # Ignore warnings.\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31ba4371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiating spark session.\n",
    "spark = SparkSession.builder.appName('recipe_reviews').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c092bac1",
   "metadata": {},
   "source": [
    "## 2. Data preparation and EDA\n",
    "- Challeging to separete them both because it will be performed back and forth.\n",
    "- First, the numerical feature will be analysed and after the written review. \n",
    "\n",
    "#### - Importing the Review dataset.\n",
    "\n",
    "To start the dataset will be imported in parquet format as cited in the introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4cc4c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Imports the dataset in a parquet format.\n",
    "df_reviews = spark.read.parquet(\"file:///home/hduser/Downloads/Dataset CA1/reviews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "500cb040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ReviewId: integer (nullable = true)\n",
      " |-- RecipeId: integer (nullable = true)\n",
      " |-- AuthorId: integer (nullable = true)\n",
      " |-- AuthorName: string (nullable = true)\n",
      " |-- Rating: integer (nullable = true)\n",
      " |-- Review: string (nullable = true)\n",
      " |-- DateSubmitted: timestamp (nullable = true)\n",
      " |-- DateModified: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Prints the Schema of the dataset.\n",
    "df_reviews.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70430e",
   "metadata": {},
   "source": [
    "- The parquet format file already had a schema, therefore keeping the original dataset schema. However, if the csv were to be handled the following code would be appropriate to apply the schema, including the column names along with the data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b9548c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code that would be used to apply the schema in the review dataset if was required.\n",
    "\n",
    "#schema_reviews = StructType([\n",
    "    #StructField('ReviewId', IntegerType(), True),\n",
    "    #StructField('RecipeId', IntegerType(), True),\n",
    "    #StructField('AuthorId', IntegerType(), True),\n",
    "    #StructField('AuthorName', StringType(), True),\n",
    "    #StructField('Rating', IntegerType(), True),\n",
    "    #StructField('Review', StringType(), True),\n",
    "    #StructField('DateSubmitted', TimestampType(), True),\n",
    "    #StructField('DateModified', TimestampType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef469f7",
   "metadata": {},
   "source": [
    "The library Pandas used in the previous semester had different set of functions and/or commands. For example, the shape which provided the number of columns and observations in the dataset. However, in the PySpark the count method is use to tell how many rows and the lenght of the columns for the number of features. \n",
    "<br>However, in a later stage pandas format will be used for visualization purpose only, because pandas visualizations are better than the pyspark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3400dc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------+----------------+------+--------------------+-------------------+-------------------+\n",
      "|ReviewId|RecipeId|AuthorId|      AuthorName|Rating|              Review|      DateSubmitted|       DateModified|\n",
      "+--------+--------+--------+----------------+------+--------------------+-------------------+-------------------+\n",
      "|       2|     992|    2008|       gayg msft|     5|better than any y...|2000-01-25 21:44:00|2000-01-25 21:44:00|\n",
      "|       7|    4384|    1634|   Bill Hilbrich|     4|I cut back on the...|2001-10-17 17:49:59|2001-10-17 17:49:59|\n",
      "|       9|    4523|    2046|Gay Gilmore ckpt|     2|i think i did som...|2000-02-25 09:00:00|2000-02-25 09:00:00|\n",
      "|      13|    7435|    1773|   Malarkey Test|     5|easily the best i...|2000-03-13 21:15:00|2000-03-13 21:15:00|\n",
      "|      14|      44|    2085|      Tony Small|     5|  An excellent dish.|2000-03-28 13:51:00|2000-03-28 13:51:00|\n",
      "|      17|    5221|    2046|Gay Gilmore ckpt|     4|love it, but with...|2000-05-08 12:08:00|2000-05-08 12:08:00|\n",
      "|      19|   13307|    2046|Gay Gilmore ckpt|     5|chewy goodness, n...|2000-05-21 17:59:00|2000-05-21 17:59:00|\n",
      "|      21|     148|    2156|  Darlene Blythe|     0|Would someone ple...|2000-06-02 11:01:00|2000-06-02 11:01:00|\n",
      "|      22|     517|    2046|Gay Gilmore ckpt|     5|thought this was ...|2000-02-25 09:02:00|2000-02-25 09:02:00|\n",
      "|      23|    4684|    2046|Gay Gilmore ckpt|     5|this is absolutel...|2000-02-25 09:06:00|2000-02-25 09:06:00|\n",
      "+--------+--------+--------+----------------+------+--------------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# It shows the first ten observations.\n",
    "df_reviews.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ecdc19",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1401982, 8)\n"
     ]
    }
   ],
   "source": [
    "num_rows_reviews = df_reviews.count() # Counting the number of rows.\n",
    "num_columns_reviews =len(df_reviews.columns) # Length of columns.\n",
    "print(f\"Shape: ({num_rows_reviews}, {num_columns_reviews})\") # Prints the shape of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1585caa0",
   "metadata": {},
   "source": [
    "The previous code shows the shape of the dataset (1401982 rows, 8 columns) and by looking at the features, a few of them will not be needed to the analysis.\n",
    "- Temporal analysis is not the focus of the project, therefore, the date submitted and modified will be dropped in an early stage.\n",
    "- The Ids will not be part of the model, but will remain to check if they can indicate any anomaly.  \n",
    "\n",
    "\n",
    "<b>Note: Multiple columns are also being deleted after ran out of memory even when reduced 10% of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e054d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Droping two columns of the dataset.\n",
    "df_reviews = df_reviews.drop(\"DateSubmitted\", \"DateModified\", \"RecipeId\", \"AuthorId\", \"ReviewId\", \"AuthorName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fd2635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations with missing Review: 0\n",
      "Number of observations with missing Rating: 0\n"
     ]
    }
   ],
   "source": [
    "# The two codes below are searching and printing the number of missing values in Review, Rating, Recipe Id.\n",
    "\n",
    "missing_review = df_reviews.filter(df_reviews.Review.isNull())\n",
    "print(f\"Number of observations with missing Review: {missing_review.count()}\")\n",
    "\n",
    "missing_rating = df_reviews.filter(df_reviews.Rating.isNull())\n",
    "print(f\"Number of observations with missing Rating: {missing_rating.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a5f718",
   "metadata": {},
   "source": [
    "- No missing values are recorded in Review and Ratings. Only performing the command for two features because they are going to be the focus of the analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "249be20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 11:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|Rating|  count|\n",
      "+------+-------+\n",
      "|     0|  76248|\n",
      "|     1|  16559|\n",
      "|     2|  17597|\n",
      "|     3|  50279|\n",
      "|     4| 229217|\n",
      "|     5|1012082|\n",
      "+------+-------+\n",
      "\n",
      "Frequency of Ratings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Checking the scores given.\n",
    "df_reviews.groupBy(\"Rating\").count().orderBy(\"Rating\").show()\n",
    "print(f\"Frequency of Ratings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d826e405",
   "metadata": {},
   "source": [
    "- The Rating columns shows that most of its values are located between 4 and 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc5d240",
   "metadata": {},
   "source": [
    "The first attempt was with the entire dataset (1401982 observations), however due to computational it was attempted many times with different sample size of the original, 100%, 70%, 50%, 20%, but erros related to \"out of memory\", notebook crashes kept happening, thus 10% of the dataset will be handled.\n",
    "- During the attempts typos in the parameters were taking hours to restart, or to wait for the error so it can be fixed later.\n",
    " \n",
    "- The sampling will occur based on the labels, which will be generated in the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d684713f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|rating|   label|\n",
      "+------+--------+\n",
      "|     5|positive|\n",
      "|     4|positive|\n",
      "|     2|negative|\n",
      "|     5|positive|\n",
      "|     5|positive|\n",
      "|     4|positive|\n",
      "|     5|positive|\n",
      "|     0|negative|\n",
      "|     5|positive|\n",
      "|     5|positive|\n",
      "|     4|positive|\n",
      "|     5|positive|\n",
      "|     5|positive|\n",
      "|     1|negative|\n",
      "|     5|positive|\n",
      "|     4|positive|\n",
      "|     1|negative|\n",
      "|     5|positive|\n",
      "|     5|positive|\n",
      "|     3| neutral|\n",
      "+------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creates a new column 'label' based on rating values\n",
    "df_reviews = df_reviews.withColumn(\"label\", \n",
    "                   when(col(\"rating\").isin([0, 1, 2]), \"negative\")\n",
    "                   .when(col(\"rating\") == 3, \"neutral\")\n",
    "                   .otherwise(\"positive\")\n",
    "                  )\n",
    "\n",
    "# You can verify by showing a few records\n",
    "df_reviews.select(\"rating\", \"label\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1938f2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 15:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Class Neutral (3): 3.586280%\n",
      "Rating Class Negatives (0-2): 7.874851%\n",
      "Rating Class Positives (4-5): 88.538869%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating the distribution of scores and the three different classes fo the train set.\n",
    "# Placing the 0,1,2 to the negatives, 3 neutral and 4,5 positives.\n",
    "class_counts = (\n",
    "    df_reviews.withColumn(\n",
    "    \"Rating_class\",\n",
    "    when((df_reviews['Rating'] >= 0) & (df_reviews['Rating'] <= 2), \"Negatives (0-2)\")\n",
    "    .when(df_reviews['Rating'] == 3, \"Neutral (3)\")\n",
    "    .otherwise(\"Positives (4-5)\"))\n",
    "# Grouping the ratings into its class and using rdd to transform them into a dictionary which is easier to access in a query.\n",
    "    .groupBy(\"Rating_class\")\n",
    "    .count()\n",
    "    .rdd.collectAsMap())\n",
    "\n",
    "# Calculates to total number of reviews.\n",
    "total_reviews = sum(class_counts.values())\n",
    "# Calculates the ratio.\n",
    "Ratios = {k: v / total_reviews for k, v in class_counts.items()}\n",
    "# Prints the percentages.\n",
    "for Rating_class, ratio in Ratios.items():\n",
    "    print(f\"Rating Class {Rating_class}: {ratio*100:2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e98963",
   "metadata": {},
   "source": [
    "- A stratified split will be aplied to maintain the original ratio, but also not taking the risk of the sample having zero neutrals.\n",
    "- The sentiment label (classes) will be used to applied the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dad2f023",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculates the fraction for each label to reduce the dataset by half\n",
    "# As mentioned previously the reduction is 50%.\n",
    "fraction = 0.01\n",
    "reduce_fractions = {label: fraction for label in df_reviews.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()}\n",
    "\n",
    "# Reducing the dataset by half\n",
    "df_reviews = df_reviews.sampleBy(\"label\", fractions=reduce_fractions, seed=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c600f026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 21:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Class Neutral (3): 3.449495%\n",
      "Rating Class Negatives (0-2): 7.973422%\n",
      "Rating Class Positives (4-5): 88.577083%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Calculating the distribution of scores and the three different classes fo the train set.\n",
    "# Placing the 0,1,2 to the negatives, 3 neutral and 4,5 positives.\n",
    "class_counts = (\n",
    "    df_reviews.withColumn(\n",
    "    \"Rating_class\",\n",
    "    when((df_reviews['Rating'] >= 0) & (df_reviews['Rating'] <= 2), \"Negatives (0-2)\")\n",
    "    .when(df_reviews['Rating'] == 3, \"Neutral (3)\")\n",
    "    .otherwise(\"Positives (4-5)\"))\n",
    "# Grouping the ratings into its class and using rdd to transform them into a dictionary which is easier to access in a query.\n",
    "    .groupBy(\"Rating_class\")\n",
    "    .count()\n",
    "    .rdd.collectAsMap())\n",
    "\n",
    "# Calculates to total number of reviews.\n",
    "total_reviews = sum(class_counts.values())\n",
    "# Calculates the ratio.\n",
    "Ratios = {k: v / total_reviews for k, v in class_counts.items()}\n",
    "# Prints the percentages.\n",
    "for Rating_class, ratio in Ratios.items():\n",
    "    print(f\"Rating Class {Rating_class}: {ratio*100:2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b8189d",
   "metadata": {},
   "source": [
    "- After applying the stratfied split, the results were successful, the difference between before and after are less than decimals.\n",
    "\n",
    "Thus, for the analysis the dataset to be handle is the 10% of the original named df_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "119234e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|           Rating|\n",
      "+-------+-----------------+\n",
      "|  count|            14147|\n",
      "|   mean|4.406729341909945|\n",
      "| stddev|1.268039054219669|\n",
      "|    min|                0|\n",
      "|    max|                5|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying statistical features of the Reviews dataset.\n",
    "df_reviews.describe().select(\"summary\", \"Rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3438b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 27:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|Rating|count|\n",
      "+------+-----+\n",
      "|     0|  751|\n",
      "|     1|  179|\n",
      "|     2|  198|\n",
      "|     3|  488|\n",
      "|     4| 2352|\n",
      "|     5|10179|\n",
      "+------+-----+\n",
      "\n",
      "Frequency of Ratings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_reviews.groupBy(\"Rating\").count().orderBy(\"Rating\").show()\n",
    "print(f\"Frequency of Ratings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52623caf",
   "metadata": {},
   "source": [
    "Based on statistical feature:\n",
    "- The total reviews are 140529;\n",
    "- It has a range of scores between 0 and 5;\n",
    "- The mean of 4.4 is close to 5, therefore most of the data points are close the highest value, so the data present signs of being left-skewed (strong evidence for skewness);\n",
    "- Also, because most of the values a concentrated in the score 5, the box (boxplot) might be very close or at the top at the upper whisker.\n",
    "- It presents a high standard deviation given the range of the scores.\n",
    "- The score zero represents the majority in the negative class, while in the positive side the maximum value is more frequent also represents more than 50% of the data.\n",
    "\n",
    "Further analysis will be performed using visual tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed45165",
   "metadata": {},
   "source": [
    "### Ratings feature visual representation.\n",
    "\n",
    "- The visualization tools using Pandas provided a better result, therefore for visualization (boxplot and histogram) purpose it will be created a dataframe using pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe7b6bbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 30:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Creating a dataframe using pandas library.\n",
    "ratings_column_pd = df_reviews.select(\"Rating\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09289558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Generating the figure.\n",
    "plt.figure(figsize=(12, 8)) # Setting the figure size.\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2) # Addind a white space on the back and adjustive font scale.\n",
    "# Creates the boxplot\n",
    "ax = sns.boxplot(data=ratings_column_pd['Rating'], width=0.5) # Boxplot, setting the width of the box to .5.\n",
    "ax.set_title('Boxplot of Rating scores') # Add title to the figure.\n",
    "plt.tight_layout() # Adjusts the layout\n",
    "plt.show() # Shows the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0215fe23",
   "metadata": {},
   "source": [
    "The boxplot shows shows that most of values are located in the maximum values, but also points the score 0, 1 and 2 as outliers, this could be due to the fact that those values represents a small percentage of the total (approximately 7-8%). So based on IQR calculations are considering them as outliers.\n",
    "\n",
    "- The values 1 and 2 will not be outliers, despite being a small amount they represent negative score reviews of the recipe;\n",
    "- The value of 0 will be analysed in a later stage along with the writen review.\n",
    "\n",
    "Despite considering of 1 and 2 score rating outliers, they will remain in the dataset. This is because they are still a valid data point and very important to achieve the goal. Although, the score zeros will be further analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec28520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating the figure for the histogram.\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Plots the histogram, bins set at 6 because of the range of the value.\n",
    "plt.hist(ratings_column_pd['Rating'], bins=6, edgecolor='black', alpha=0.7)\n",
    "plt.title(\"Histogram of Ranting Scores\") # Sets title.\n",
    "plt.xlabel(\"Ratings\") # Sets x label.\n",
    "plt.xticks(range(6)) # Add ticks with range between 0 and 6.\n",
    "plt.grid(axis='y') \n",
    "plt.show() # Display the histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01007703",
   "metadata": {},
   "source": [
    "An extra visualization to look at the distribution of the data, the histogram shows that the ratings scores data is highly skewed to the left and most of its values are located at the maximum score.\n",
    "\n",
    "The scores zero raised doubts about its reliability, potentially someone forgot to leave a score or if it is actually the worst score. Also, there are no notes about the score zero, so a manual check will be applied to check if there is a balance between the review and the score zero. The procedure will be done by looking at random samples (attempted multiple times with differend seed values), checking if the reviews truly represents a sentiment of a lowest score possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ef49528",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Review                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|I haven't yet tried this recipe,but a comment on all of the sausage recipes I've seen on Recipezaar. There are several which call for fennel,but none for anise. Try subing anise for fennel.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|It is a recipe that my mother passed down to me, I'm really not sure where it originated from. \\r\\nI love it with the black olives too but my husband isn't a fan of the olives so we have it without. It really is good with any meal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|This is a great way to cook brisket, but please don't call it BBQ.  I am THE past master of Southwest Texas Bar-B-que and I have been doing it for over 50 years now and have fed as many as 200 at a time.  They end up putting my BBQ sauce all over everything on their plate and sop it all up with their bread.  Warren                                                                                                                                                                                                                                                                                                                                                                                                                                 |\n",
      "|By the way, you can use 2 larger cans of cherry filling.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|It's funny how differntly we can each perceive a recipe!  :)  I found these to be very, very good... but almost too sweet.  I followed the recipe exactly, but next time I think I'll begin cutting back the syrup by at least 1/4 cup.  That said, there is nothing to compare to the taste of fresh granola, and I will indeed be trying these very soon!                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|Barbara, I'd like to try this but it seems that vinegar or something is missing or does the canning salt add the \"zing\"?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|As \"SherryZ\" pointed out,\\r\\n\\r\\nthis is not really the way we make them in Sweden, although the recipe is closer to the original than the one by \"SueL\", which has no resemblance at all with the original.\\r\\n\\r\\nIn Sweden, we never use baking powder and very seldom sugar in the batter.                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|>I have not made this dish, but it is in my files to try.\\r\\n\\r\\nWhy bother trying an untested recipie?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|I finally located a copy of Mr. Smith's recipe, for those who may want to know the differences between his and my own.\\r\\nHe seems to include zucchini as one of his ingredients, and omits the chives that I find give a nice flavor. He does have different quantities on every single thing that can be used, although the general cooking method is about the same, as I would assume would be for all bubble and squeak recipes.\\r\\nAs I have said before, this is a traditional recipe, where many versions exist, even beyond these. I encourage you to try any version that pleases you best! :)                                                                                                                                                     |\n",
      "|When you add water while these are cooking, be sure it is very hot or the beans will have a tendency to burst.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|I don't want to rate this since I haven't followed this exact recipe but I can say this is pretty much how we do our asparagus roll-ups, except we used to buy canned asparagus because the fresh often wasn't available. I bet it's even better with the real thing!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|Have also made this adding 1teasp minced garlic & 1teasp grated fresh ginger to the pork marinade just to kick it up a bit more.  Great this way too.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|I'll personally vouch for this recipe (I posted a similar one... \"Easy Mediterranean Pizza\").  Make sure to try one of our recipes out... you won't regret it!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|i haven't tried this recipe yet as i just got it from this website. i am about the give it a try now and am very sure i will like it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|This also makes a wonderful 'deep dish' pizza crust as pictured above.  The recipe will make one deep dish crust.\\nBrush rimmed baking sheet with olive oil, place dough into pan and shape and dimple with finger tips.\\nBrush top with olive oil and pre-bake at 425 F for 7 minutes - then add toppings and bake for another 10-15 minutes.                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|This is one of those recipes..you love it or hate it...It was not for us [even my chicken loven cat would not eat it] But this is just our taste.\\r\\nTip!!!If you want to brave it, brown chicken\\r\\nvery well about till done..than proceed..other\\r\\nwise garlic & peppers burn.                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|I would not make this again.  My daugter would not take a second bite and I would not make it again. \\r\\n\\r\\nI once made a recipe with Kraft Mac & Cheese, browned beef, and some barbeque sauce that was pretty good and a lot simpler and tastier than this recipe.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|I made this this evening as it is Oman's 32nd National Day and we had guests coming over. I wanted to try something new- had a fresh bottle of honey on hand and exactly 2 cups of flour- voila, I just had to get going in the kitchen, ofcourse:)\\r\\nI baked this in my AMC Dutch Oven from 5:15pm-6:30pm. It was still not baked! So, I kept it covered and opened it at 9pm - It was DONE! BAKED! Luckily for me, our guests turned up late and got to have a huge slice each of this delicious bread! My dad loves the fact that this loaf is eggless and butter/margarine-less as well! It's a BIG achievement for me to finally bake something successfully without eggs and fat!\\r\\nDoing an endless happy dance today and smiling from ear to ear :)|\n",
      "|KORMA is an Indian dish. I don't know if it was a mistype or just a simple mistake but I thought you might like to know what it is trtuly called. I come from an Indian background and grgew up loving Korma.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |\n",
      "|Try using Splenda instead of sugar to reduce calories. Carole in Orlando                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "zero_reviews = df_reviews.filter(col(\"Rating\") == 0)\n",
    "sample_reviews = zero_reviews # Also used numerous seeds.\n",
    "sample_reviews.select(\"Review\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de085cbd",
   "metadata": {},
   "source": [
    "- According to last code (with many attempts using different seed numbers), the written reviews does not match the score zero given. Positives reviews such as, great recipe, its delicious . Therefore, the chosen option is to drop the 0 scores (anomalies), so it does not affects the analysis by introducing noise to it, increasing the consistency in the data.\n",
    "- And since they are anomalies in the data, the rows with score of 0 will be dropped.\n",
    "<br><br>The next two codes will be used to drop (by using the method filter) and after printing the counts before and after the operation to check if it was correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08132f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method filter to exclude the rows with Rating equal to zero.\n",
    "df_reviews = df_reviews.filter(df_reviews.Rating != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b965be92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 32:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After filter obs count: 13396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Counting after the exclusion and printing them for verification. \n",
    "filtered_count = df_reviews.count()\n",
    "print(f\"After filter obs count: {filtered_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37fd6f70",
   "metadata": {},
   "source": [
    "- After the drop, the observations reduced by 7676, which is the number of zero scores given (also provided at the frequency line of code). Therefore, the drop operation was sucessful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0057ccc2",
   "metadata": {},
   "source": [
    "## 3. Text pre-processing\n",
    "\n",
    "Next stage will be the text processing of the Review column to handle any characters that are not processed nor have meaning.\n",
    "\n",
    "- Lowering case; \n",
    "- Removing punctuations and \\r\\n;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67702a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a column named Review_lower to the dataframe in a lowercase version.\n",
    "df_reviews = df_reviews.withColumn(\"Review_lower\", lower(col(\"Review\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c4de4a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 35:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformation successful.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Looks for any observation that the transformation did not work.\n",
    "no_match = df_reviews.filter(lower(col(\"Review\")) !=col(\"Review_lower\")).count()\n",
    "\n",
    "if no_match == 0:\n",
    "    print(\"Transformation successful.\")\n",
    "else:\n",
    "    print(\"There is/are rows that the transformation did not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed3294",
   "metadata": {},
   "source": [
    "- After the transformation, a few statistical features will be generated before further processing for comparison purpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5bdbca73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 38:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+\n",
      "|summary|        word_count|\n",
      "+-------+------------------+\n",
      "|  count|             13396|\n",
      "|   mean| 53.38981785607644|\n",
      "| stddev|37.353617614270384|\n",
      "|    min|                 1|\n",
      "|    max|               465|\n",
      "+-------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# It sums the number of words in the review column.\n",
    "df_reviews = df_reviews.withColumn(\"word_count\", size(split(df_reviews[\"Review_lower\"], \" \")))\n",
    "# Display statistical features of the review number of words.\n",
    "df_reviews.select(\"word_count\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0afc4",
   "metadata": {},
   "source": [
    "According to the descriptive statistics of the review (132853 in total):\n",
    "- It ranges from 1 word to 944 words;\n",
    "- The mean is 53.35, thus the average of words is 53 words per review.\n",
    "- Most of the reviews are likely to be clustered in the values close to 53 words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fd3802fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 41:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rating Class Positive: 93.54%\n",
      "Rating Class Neutral: 3.64%\n",
      "Rating Class Negative: 2.81%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Group by the label column to get counts\n",
    "class_counts = df_reviews.groupBy(\"label\").count().rdd.collectAsMap()\n",
    "\n",
    "# Calculate the total number of reviews\n",
    "total_reviews = sum(class_counts.values())\n",
    "\n",
    "# Calculate the ratios\n",
    "Ratios = {k: v / total_reviews for k, v in class_counts.items()}\n",
    "\n",
    "# Print the percentages\n",
    "for label, ratio in Ratios.items():\n",
    "    print(f\"Rating Class {label.capitalize()}: {ratio*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbeddff3",
   "metadata": {},
   "source": [
    "The reviews have gone through some manipulation and the last line of code still shows that.\n",
    "\n",
    "- By far, the highest number of score given is between the value of 4 and 5;\n",
    "- Lower scores (1 , 2) represents less than 3% of the total of ratings given and less than 4% for neutral scores (3);\n",
    "\n",
    "Next pre processing stage is to remove anything that is not a letter and characters such as \\r\\n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4b0b4349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing punctions of the Review_lowercase column.\n",
    "df_reviews = df_reviews.withColumn(\"Review_no_punct\", regexp_replace(col(\"Review_lower\"), \"[^a-zA-Z\\s]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e5361a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Review_lower                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |Review_no_punct                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|average taste. not particularly subtle                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |average taste not particularly subtle                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|this recipe was delicious, but took a lot of time, cost and effort to make.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |this recipe was delicious but took a lot of time cost and effort to make                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|i had never cooked a corned beef before and wanted a recipe to serve family for st. patrick's day. found this recipe via the internet and tried it. everyone at dinner raved about the tenderness of the meat and the flavor. they continue to talk about how good it was. the recipe was easy to follow and everything turned out perfect. in fact, they asked for the recipe.                                                                                                                                                                                                                                                                                                                                                                                                      |i had never cooked a corned beef before and wanted a recipe to serve family for st patricks day found this recipe via the internet and tried it everyone at dinner raved about the tenderness of the meat and the flavor they continue to talk about how good it was the recipe was easy to follow and everything turned out perfect in fact they asked for the recipe                                                                                                                                                                                                                                                                                                                                                                                            |\n",
      "|the bread itself is quite good, although alittle bitter in taste.\\r\\nthe crust is excellant. if anyone has a recipe for jewish black bread for a bread machine i would love to have it.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |the bread itself is quite good although alittle bitter in taste\\r\\nthe crust is excellant if anyone has a recipe for jewish black bread for a bread machine i would love to have it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|this is one very easy way indeed to make \"sweet and sour chicken\". really liked this byt i think that we should add some onion paste as well at the time of baking to the sauce as i did and it really turned out very well.\\r\\nsmiles\\r\\nmini                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |this is one very easy way indeed to make sweet and sour chicken really liked this byt i think that we should add some onion paste as well at the time of baking to the sauce as i did and it really turned out very well\\r\\nsmiles\\r\\nmini                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|easiest recipe made even easier-- since i have found that ore-ida makes \"potatoes o'brian\" which already have diced peppers and onion with the hash browns,so that really saves some time.  i also use cream of celery soup (instead of the mushroom) so no need to add the chopped  celery- it is great!                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |easiest recipe made even easier since i have found that oreida makes potatoes obrian which already have diced peppers and onion with the hash brownsso that really saves some time  i also use cream of celery soup instead of the mushroom so no need to add the chopped  celery it is great                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|like most apple and banana drinks, i loved this one too. never realised how well bananas and apples go together! had to make it without the vanilla extract though... had none left at home :(                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |like most apple and banana drinks i loved this one too never realised how well bananas and apples go together had to make it without the vanilla extract though had none left at home                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|the pig's apron is a problem. i assume it means a pig's caul which is a stomach lining and even the food trade is finding it hard to get this now. it requires the slaughter house to part with it, and in these days of bse the \"dodgy\" bits of animals are not permitted by law to pass into the food industry chain. \\r\\n\\r\\nthe only people who make faggots with caul wrapped around them are people who have access to the killing of the pigs.  so i don't know if an egg to \"bind\" or some other thing like the suet will cause them to stick together like sausages for easier cutting on the plate.  without it many faggots just disintegrate and are not so good when that happens.   rice paper might be a substitute but it's a shame you can't get a synthetic \"caul\".|the pigs apron is a problem i assume it means a pigs caul which is a stomach lining and even the food trade is finding it hard to get this now it requires the slaughter house to part with it and in these days of bse the dodgy bits of animals are not permitted by law to pass into the food industry chain \\r\\n\\r\\nthe only people who make faggots with caul wrapped around them are people who have access to the killing of the pigs  so i dont know if an egg to bind or some other thing like the suet will cause them to stick together like sausages for easier cutting on the plate  without it many faggots just disintegrate and are not so good when that happens   rice paper might be a substitute but its a shame you cant get a synthetic caul|\n",
      "|i'm from yorkshire, i still cook yorkshire puddings for my american family on a regular basis, my family recipe had approx 1/4 cup of hot tap water added it helps them rise better.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |im from yorkshire i still cook yorkshire puddings for my american family on a regular basis my family recipe had approx  cup of hot tap water added it helps them rise better                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|i have made this roast beef for several people already,it is perfect.the only thing i changed was,i made my own gravy from a powder.it is delicious.         nora                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |i have made this roast beef for several people alreadyit is perfectthe only thing i changed wasi made my own gravy from a powderit is delicious         nora                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|lemon bars was easy and was a hit with my guest.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |lemon bars was easy and was a hit with my guest                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|i love simple and easy.  this was prepared in the early am and cooked in the pm.  great stuff                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |i love simple and easy  this was prepared in the early am and cooked in the pm  great stuff                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|this is a wonderful recipe.  i used boneless chicken breasts cut in strips, served over egg noodles. the mixture of wine and sour cream gave it a savory taste.  and easy to make, just threw it all in the crockpot!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                |this is a wonderful recipe  i used boneless chicken breasts cut in strips served over egg noodles the mixture of wine and sour cream gave it a savory taste  and easy to make just threw it all in the crockpot                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|this was very tasty.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |this was very tasty                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|excellent!  lindsey & i made this with eli totz' unemployment potatoes and it was a perfect fried meal.  only complaint was the batter didn't stick to the chicken all that well (the corn flakes fell off in the oil during frying) but neither of us knows how to fix that.  definitely a 5-star recipe.                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |excellent  lindsey  i made this with eli totz unemployment potatoes and it was a perfect fried meal  only complaint was the batter didnt stick to the chicken all that well the corn flakes fell off in the oil during frying but neither of us knows how to fix that  definitely a star recipe                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |\n",
      "|i made this dish once using shrimp and later beef. i have made it regularly ever since, freezing portions, then reheating them in the microwave in their plastic containers as a breakfast entree or snack. i find it convenient, fast, nutritious and tasty. add a piece of fruit and you have a good breakfast.  -freeman price-                                                                                                                                                                                                                                                                                                                                                                                                                                                   |i made this dish once using shrimp and later beef i have made it regularly ever since freezing portions then reheating them in the microwave in their plastic containers as a breakfast entree or snack i find it convenient fast nutritious and tasty add a piece of fruit and you have a good breakfast  freeman price                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|this dish is excellent. i have made it twice now and the second time i didn't have any ricotta cheese, so i used cottage cheese which was also good.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |this dish is excellent i have made it twice now and the second time i didnt have any ricotta cheese so i used cottage cheese which was also good                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|great recipe.  i don't know jimmy dean so used hot italian and they were very good. thanks diana                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |great recipe  i dont know jimmy dean so used hot italian and they were very good thanks diana                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|excellent punch.  we didn't try it out before hand, but it received rave reviews from all the attendees.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |excellent punch  we didnt try it out before hand but it received rave reviews from all the attendees                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|i made this and had much better luck. it reminds me quite a bit of gumbo. i did add lots of hot sauce.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |i made this and had much better luck it reminds me quite a bit of gumbo i did add lots of hot sauce                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displays the rows of column before and after the removal of punctuations. \n",
    "df_reviews.select(\"Review_lower\", \"Review_no_punct\").show(n=20, truncate=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b1f1c5",
   "metadata": {},
   "source": [
    "- The previous line of code shows that the removal of puctuations was successful. However, it shows special characters like \\r\\n, which will be removed in the following step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e037c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing \\r\\n.\n",
    "df_reviews = df_reviews.withColumn(\"Review_no_punct2\", regexp_replace(col(\"Review_no_punct\"), \"[\\r\\n]\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf53c319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Review_no_punct                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |Review_no_punct2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|average taste not particularly subtle                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |average taste not particularly subtle                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|this recipe was delicious but took a lot of time cost and effort to make                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |this recipe was delicious but took a lot of time cost and effort to make                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|i had never cooked a corned beef before and wanted a recipe to serve family for st patricks day found this recipe via the internet and tried it everyone at dinner raved about the tenderness of the meat and the flavor they continue to talk about how good it was the recipe was easy to follow and everything turned out perfect in fact they asked for the recipe                                                                                                                                                                                                                                                                                                                                                                                            |i had never cooked a corned beef before and wanted a recipe to serve family for st patricks day found this recipe via the internet and tried it everyone at dinner raved about the tenderness of the meat and the flavor they continue to talk about how good it was the recipe was easy to follow and everything turned out perfect in fact they asked for the recipe                                                                                                                                                                                                                                                                                                                                                                                    |\n",
      "|the bread itself is quite good although alittle bitter in taste\\r\\nthe crust is excellant if anyone has a recipe for jewish black bread for a bread machine i would love to have it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |the bread itself is quite good although alittle bitter in tastethe crust is excellant if anyone has a recipe for jewish black bread for a bread machine i would love to have it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|this is one very easy way indeed to make sweet and sour chicken really liked this byt i think that we should add some onion paste as well at the time of baking to the sauce as i did and it really turned out very well\\r\\nsmiles\\r\\nmini                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |this is one very easy way indeed to make sweet and sour chicken really liked this byt i think that we should add some onion paste as well at the time of baking to the sauce as i did and it really turned out very wellsmilesmini                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |\n",
      "|easiest recipe made even easier since i have found that oreida makes potatoes obrian which already have diced peppers and onion with the hash brownsso that really saves some time  i also use cream of celery soup instead of the mushroom so no need to add the chopped  celery it is great                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |easiest recipe made even easier since i have found that oreida makes potatoes obrian which already have diced peppers and onion with the hash brownsso that really saves some time  i also use cream of celery soup instead of the mushroom so no need to add the chopped  celery it is great                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|like most apple and banana drinks i loved this one too never realised how well bananas and apples go together had to make it without the vanilla extract though had none left at home                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |like most apple and banana drinks i loved this one too never realised how well bananas and apples go together had to make it without the vanilla extract though had none left at home                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
      "|the pigs apron is a problem i assume it means a pigs caul which is a stomach lining and even the food trade is finding it hard to get this now it requires the slaughter house to part with it and in these days of bse the dodgy bits of animals are not permitted by law to pass into the food industry chain \\r\\n\\r\\nthe only people who make faggots with caul wrapped around them are people who have access to the killing of the pigs  so i dont know if an egg to bind or some other thing like the suet will cause them to stick together like sausages for easier cutting on the plate  without it many faggots just disintegrate and are not so good when that happens   rice paper might be a substitute but its a shame you cant get a synthetic caul|the pigs apron is a problem i assume it means a pigs caul which is a stomach lining and even the food trade is finding it hard to get this now it requires the slaughter house to part with it and in these days of bse the dodgy bits of animals are not permitted by law to pass into the food industry chain the only people who make faggots with caul wrapped around them are people who have access to the killing of the pigs  so i dont know if an egg to bind or some other thing like the suet will cause them to stick together like sausages for easier cutting on the plate  without it many faggots just disintegrate and are not so good when that happens   rice paper might be a substitute but its a shame you cant get a synthetic caul|\n",
      "|im from yorkshire i still cook yorkshire puddings for my american family on a regular basis my family recipe had approx  cup of hot tap water added it helps them rise better                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |im from yorkshire i still cook yorkshire puddings for my american family on a regular basis my family recipe had approx  cup of hot tap water added it helps them rise better                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|i have made this roast beef for several people alreadyit is perfectthe only thing i changed wasi made my own gravy from a powderit is delicious         nora                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |i have made this roast beef for several people alreadyit is perfectthe only thing i changed wasi made my own gravy from a powderit is delicious         nora                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |\n",
      "|lemon bars was easy and was a hit with my guest                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |lemon bars was easy and was a hit with my guest                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|i love simple and easy  this was prepared in the early am and cooked in the pm  great stuff                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |i love simple and easy  this was prepared in the early am and cooked in the pm  great stuff                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |\n",
      "|this is a wonderful recipe  i used boneless chicken breasts cut in strips served over egg noodles the mixture of wine and sour cream gave it a savory taste  and easy to make just threw it all in the crockpot                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |this is a wonderful recipe  i used boneless chicken breasts cut in strips served over egg noodles the mixture of wine and sour cream gave it a savory taste  and easy to make just threw it all in the crockpot                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|this was very tasty                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |this was very tasty                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "|excellent  lindsey  i made this with eli totz unemployment potatoes and it was a perfect fried meal  only complaint was the batter didnt stick to the chicken all that well the corn flakes fell off in the oil during frying but neither of us knows how to fix that  definitely a star recipe                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |excellent  lindsey  i made this with eli totz unemployment potatoes and it was a perfect fried meal  only complaint was the batter didnt stick to the chicken all that well the corn flakes fell off in the oil during frying but neither of us knows how to fix that  definitely a star recipe                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "|i made this dish once using shrimp and later beef i have made it regularly ever since freezing portions then reheating them in the microwave in their plastic containers as a breakfast entree or snack i find it convenient fast nutritious and tasty add a piece of fruit and you have a good breakfast  freeman price                                                                                                                                                                                                                                                                                                                                                                                                                                          |i made this dish once using shrimp and later beef i have made it regularly ever since freezing portions then reheating them in the microwave in their plastic containers as a breakfast entree or snack i find it convenient fast nutritious and tasty add a piece of fruit and you have a good breakfast  freeman price                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|this dish is excellent i have made it twice now and the second time i didnt have any ricotta cheese so i used cottage cheese which was also good                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |this dish is excellent i have made it twice now and the second time i didnt have any ricotta cheese so i used cottage cheese which was also good                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "|great recipe  i dont know jimmy dean so used hot italian and they were very good thanks diana                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |great recipe  i dont know jimmy dean so used hot italian and they were very good thanks diana                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             |\n",
      "|excellent punch  we didnt try it out before hand but it received rave reviews from all the attendees                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |excellent punch  we didnt try it out before hand but it received rave reviews from all the attendees                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |\n",
      "|i made this and had much better luck it reminds me quite a bit of gumbo i did add lots of hot sauce                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |i made this and had much better luck it reminds me quite a bit of gumbo i did add lots of hot sauce                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the results of the last operation.\n",
    "df_reviews.select(\"Review_no_punct\", \"Review_no_punct2\").show(n=20, truncate=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e12e438",
   "metadata": {},
   "source": [
    "- The last code showed that \\r\\n characters also were removed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5922d785",
   "metadata": {},
   "source": [
    "Next step will separete the sentence into words (tokens)\n",
    "- The tokenizer provided by Keras has an additional feature which is possible to set the maximum number of words, however in the Spark enviroment this feature is not available. Therefore, a different approach to use a maximum range of vocabulary will be applied in later stage. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2826257",
   "metadata": {},
   "source": [
    "### Tokenizing all sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89820227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"Review_no_punct2\", outputCol=\"tokens\")\n",
    "\n",
    "# Tokenize X_train\n",
    "df_reviews = tokenizer.transform(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2adf771e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 46:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| words|count|\n",
      "+------+-----+\n",
      "|      |32779|\n",
      "|     i|31576|\n",
      "|   the|30831|\n",
      "|   and|23510|\n",
      "|     a|18197|\n",
      "|    it|16126|\n",
      "|  this|14135|\n",
      "|    to|14047|\n",
      "|   for|13449|\n",
      "|    of|11489|\n",
      "|   was| 9844|\n",
      "|recipe| 7982|\n",
      "|    in| 6628|\n",
      "|    my| 6461|\n",
      "|  with| 6417|\n",
      "|    is| 5866|\n",
      "|    so| 5771|\n",
      "|  made| 5543|\n",
      "|  used| 5127|\n",
      "|   but| 4722|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# It \"explodes\" the tokens column to have one token per row.\n",
    "df_reviews_exploded = df_reviews.select(explode(\"tokens\").alias(\"words\"))\n",
    "\n",
    "# Groups individual by tokens and count their occurrences.\n",
    "word_counts = df_reviews_exploded.groupBy(\"words\").agg(count(\"words\").alias(\"count\"))\n",
    "\n",
    "# Orders by count in descending order to see most frequent words at the top.\n",
    "word_counts_ordered = word_counts.orderBy(desc(\"count\"))\n",
    "\n",
    "# Show the results.\n",
    "word_counts_ordered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2b9e13",
   "metadata": {},
   "source": [
    "- As shown in the previous cell that there are spaces being counted which does not have any meaning to the model. The removal of this character will be filtering out short tokens (less than 2), the reason for the number is that word like \"no\".\n",
    "\n",
    "Thus, the next two lines of code will filter out short tokens with lenght that are less than 2 characters, including the spaces. \n",
    "\n",
    "### Filter short tokens (less than 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e9da725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDF to filter out short tokens in all three sets.\n",
    "filter_short_udf = udf(lambda tokens: [token for token in tokens if len(token) > 1], ArrayType(StringType()))\n",
    "\n",
    "df_reviews = df_reviews.withColumn(\"filt_tokens\", filter_short_udf(df_reviews[\"tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16e37012",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| words|count|\n",
      "+------+-----+\n",
      "|   the|30831|\n",
      "|   and|23510|\n",
      "|    it|16126|\n",
      "|  this|14135|\n",
      "|    to|14047|\n",
      "|   for|13449|\n",
      "|    of|11489|\n",
      "|   was| 9844|\n",
      "|recipe| 7982|\n",
      "|    in| 6628|\n",
      "|    my| 6461|\n",
      "|  with| 6417|\n",
      "|    is| 5866|\n",
      "|    so| 5771|\n",
      "|  made| 5543|\n",
      "|  used| 5127|\n",
      "|   but| 4722|\n",
      "|thanks| 4096|\n",
      "|  that| 4087|\n",
      "|  make| 4012|\n",
      "+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Explode the tokens column to have one token per row, group by words, and count their occurrences.\n",
    "word_counts_ordered = df_reviews.select(explode(\"filt_tokens\").alias(\"words\")) \\\n",
    "    .groupBy(\"words\") \\\n",
    "    .agg(count(\"words\").alias(\"count\")) \\\n",
    "    .orderBy(desc(\"count\"))\n",
    "\n",
    "# Show the results.\n",
    "word_counts_ordered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "297ba348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert the word counts to pandas dataframe for visualization\n",
    "word_counts_ordered_pd = word_counts_ordered.toPandas()\n",
    "\n",
    "# Compute the cumulative sum of the word frequencies\n",
    "word_counts_ordered_pd['cumulative_freq'] = word_counts_ordered_pd['count'].cumsum()\n",
    "word_counts_ordered_pd['cumulative_freq'] /= word_counts_ordered_pd['count'].sum()  # normalizes to get the CDF\n",
    "\n",
    "# Plot the CDF\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(word_counts_ordered_pd['cumulative_freq'], marker='.', linestyle='none')\n",
    "plt.xlabel('Word Rank')\n",
    "plt.xlim(0, 20000)\n",
    "plt.ylabel('CDF')\n",
    "plt.title('CDF of Word Frequencies')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51dd5e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total word occurrences: 630099\n",
      "Occurrences of top 15000 words: 625867\n",
      "Coverage of top 15000 words: 99.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Compute total word occurrences\n",
    "total_word_occurrences = word_counts_ordered.agg({\"count\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "# Compute occurrences of top N words\n",
    "N = 15000\n",
    "top_N_word_occurrences = word_counts_ordered.limit(N).agg({\"count\": \"sum\"}).collect()[0][0]\n",
    "\n",
    "# Compute the coverage\n",
    "coverage = (top_N_word_occurrences / total_word_occurrences) * 100\n",
    "\n",
    "print(\"Total word occurrences:\", total_word_occurrences)\n",
    "print(f\"Occurrences of top {N} words:\", top_N_word_occurrences)\n",
    "print(f\"Coverage of top {N} words: {coverage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f04b9dc",
   "metadata": {},
   "source": [
    "- 20000 words is the number that represents 98.97% in the reviews. Thus, due to memory reasons and performance, the maximum vocabulary will be determined by the train set and them the reviews will be filtered out based on the X number of words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2877c434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Gets the top N tokens\n",
    "N = 15000\n",
    "top_N_tokens = word_counts_ordered.limit(N).select(\"words\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Broadcasting the top tokens list for efficient filtering\n",
    "top_tokens_broadcast = spark.sparkContext.broadcast(set(top_N_tokens))\n",
    "\n",
    "# Defines a filtering function that retains only those tokens present in our broadcasted top tokens set\n",
    "def filter_native(tokens):\n",
    "    return [t for t in tokens if t in top_tokens_broadcast.value]\n",
    "\n",
    "# Defining UDF for our function\n",
    "filter_udf = udf(filter_native, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to the tokenized column in all three datasets:\n",
    "df_reviews = df_reviews.withColumn(\"filtered_tokens\", filter_udf(df_reviews[\"tokens\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "de5fa2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 72:>                                                         (0 + 2) / 2]\r",
      "\r",
      "[Stage 72:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct tokens in the training set (reviews): 15000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "distinct_token_count = (df_reviews\n",
    "                        .select(explode(col(\"filtered_tokens\")).alias(\"word\"))\n",
    "                        .distinct()\n",
    "                        .count())\n",
    "\n",
    "print(f\"Total distinct tokens in the training set (reviews): {distinct_token_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a08252",
   "metadata": {},
   "source": [
    "- Next transformation to be called is to remove the stop words, a step that can be performed in early stages, although on Spark, setences has to be separeted by words. Thus this is why this step is being used now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d136c2e5",
   "metadata": {},
   "source": [
    "### Removing StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad0b817a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines the stopword remover.\n",
    "remover = StopWordsRemover(inputCol=\"filtered_tokens\", outputCol=\"t_no_stopwords\")\n",
    "\n",
    "# Removes stopwords from the training dataset.\n",
    "df_reviews = remover.transform(df_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a973f8a",
   "metadata": {},
   "source": [
    "- After some stages of pre processing (mainly transformations) the reviews, it could potentially create empty cells, for example a review with just punctuations or with just stopwords. Therefore, the following code will search for empty reviews in all three datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "abf8b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 78:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty reviews : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Counts the number of empty reviews.\n",
    "empty_count = df_reviews.filter(size(df_reviews[\"t_no_stopwords\"]) == 0).count()\n",
    "\n",
    "# Prints the results.\n",
    "print(f\"Number of empty reviews : {empty_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd174012",
   "metadata": {},
   "source": [
    "- The results of the last code shows that all the three sets now have a few empty cells, that might have been transformed to empty due to transformations. Therefore, the chosen option will be to drop the empty ones because it could potentially impact model efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b152b3e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Filter out empty reviews in df_reviews\n",
    "df_reviews = df_reviews.filter(size(df_reviews[\"t_no_stopwords\"]) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b35c33",
   "metadata": {},
   "source": [
    "- For computational resources (time consuming), the maximum length of words (the vocabulary) is going to be taken from the train set only, by performing word count and frequency, along with visualization to help identify what is the number that represents the majority (at least 90%) of the words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5f84e00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 81:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of empty reviews : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Counts the number of empty reviews.\n",
    "empty_count = df_reviews.filter(size(df_reviews[\"t_no_stopwords\"]) == 0).count()\n",
    "\n",
    "# Prints the results.\n",
    "print(f\"Number of empty reviews : {empty_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348879b5",
   "metadata": {},
   "source": [
    "The last code shows that there are no more emty reviews.\n",
    "- Lemmatization will be applied to handle words for example that are in the past, or with ing, but it has the same representation as the verb or noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9d49f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading spaCy model for Lemmatization.\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\", \"textcat\"])\n",
    "\n",
    "# Defines a Pandas UDF to perform lemmatization on a column of tokens.\n",
    "@pandas_udf(ArrayType(StringType()))\n",
    "def lemmatize_udf(tokens_series):\n",
    "    # Functions to lemmatize a list of tokens using spaCy.\n",
    "    def lemmatize(tokens):\n",
    "        doc = nlp(\" \".join(tokens))\n",
    "        return [token.lemma_ for token in doc]\n",
    "\n",
    "    # Appling the lemmatization function to each row in the input column.\n",
    "    return tokens_series.apply(lemmatize)\n",
    "\n",
    "# Appling the Pandas UDF to the DataFrame.\n",
    "df_reviews = df_reviews.withColumn(\"lemmatized_tokens\", lemmatize_udf(df_reviews[\"t_no_stopwords\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8015b884",
   "metadata": {},
   "source": [
    "- After the stage of processing, the dataset will be split into three different sets, the training, validation and test in order to perform functions without causing any data leakage. \n",
    "- The ratio between classes observed a few steps back (i.e: Positives - above 93%), and because of this factor the stratified method will be aplied. When splitting the dataset into three, there is a chance of one of the sets do not get any values for neutral or/and negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a0549c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining fractions to split the dataset.\n",
    "train_fractions = {label: 0.6 for label in df_reviews.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()}\n",
    "\n",
    "# Creating a train set.\n",
    "df_train = df_reviews.sampleBy(\"label\", fractions=train_fractions, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "48fd3255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the 40% left set to be splitted between validation and test.\n",
    "remaining_df = df_reviews.subtract(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "984df1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Defining fractions to split the dataset.\n",
    "valid_fractions = {label: 0.5 for label in df_reviews.select(\"label\").distinct().rdd.flatMap(lambda x: x).collect()}\n",
    "\n",
    "# Creates the validation set from the 40%, using its franction of \n",
    "df_valid = remaining_df.sampleBy(\"label\", fractions=valid_fractions, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "be760bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test df. \n",
    "df_test = remaining_df.subtract(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeefe68",
   "metadata": {},
   "source": [
    "- The division of the set were training have 60%, validation 20% and test 20%. Next, will be performed one-hot-enconder. The model only accepts numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adbb645e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# StringIndexer to convert labels to numerical values\n",
    "label_indexer = StringIndexer(inputCol=\"label\", outputCol=\"label_index\")\n",
    "\n",
    "# OneHotEncoder to convert numerical values to one-hot encoded vectors\n",
    "label_encoder = OneHotEncoder(inputCol=\"label_index\", outputCol=\"label_encoded\")\n",
    "\n",
    "# Create a pipeline to execute both operations\n",
    "label_pipeline = Pipeline(stages=[label_indexer, label_encoder])\n",
    "\n",
    "# Fit and transform the training data\n",
    "label_model = label_pipeline.fit(df_train)\n",
    "df_train = label_model.transform(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0764fb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the validation using the same model\n",
    "df_valid = label_model.transform(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25f76252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the est data using the same model\n",
    "df_test = label_model.transform(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aeabfc8",
   "metadata": {},
   "source": [
    "- In this step, will be displayed the maximum length of tokens in order to pad the shorter reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a40742b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a funtion.\n",
    "def vector_length(vector):\n",
    "    return len(vector)\n",
    "\n",
    "length_udf = udf(vector_length, IntegerType())\n",
    "\n",
    "df_train = df_train.withColumn(\"sequence_length\", length_udf(col(\"lemmatized_tokens\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a3a004bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 93:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "max_length = df_train.agg(max(col(\"sequence_length\"))).collect()[0][0]\n",
    "print(max_length)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f331a69d",
   "metadata": {},
   "source": [
    "### 4. Feature Engineering (Word2Vec and Padding)\n",
    "\n",
    "In this stage feature engineering steps will be applied to the sets.\n",
    "\n",
    "Word2Vec, the chosen tool, will transform tokens into vectors, with the size of 50 and a minimum count of 5 (it has to pop up at leat 5 times). The model will follow three steps, first will set its configurations, second model will ve trained on the train set, and third will be applied (transform) on all three sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1e54eb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.functions import udf, explode, col\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "# 1. Train Word2Vec model\n",
    "word2Vec = Word2Vec(vectorSize=50, minCount=5, inputCol=\"lemmatized_tokens\", outputCol=\"temporary_vec\")\n",
    "model = word2Vec.fit(df_train)\n",
    "\n",
    "# 2. Get the vocabulary and corresponding vectors from the model\n",
    "vocab_broadcast = spark.sparkContext.broadcast(model.getVectors().rdd.collectAsMap())\n",
    "\n",
    "# 3. Define a UDF to map each token to its vector\n",
    "def tokens_to_vectors(tokens):\n",
    "    vocab = vocab_broadcast.value\n",
    "    return [vocab[token] for token in tokens if token in vocab]\n",
    "\n",
    "tokens_to_vectors_udf = udf(tokens_to_vectors, ArrayType(VectorUDT()))\n",
    "\n",
    "# 4. Transform the DataFrame using the UDF\n",
    "def transform_dataset(dataset):\n",
    "    return (dataset\n",
    "            .withColumn(\"Review_Vec_Sequence\", tokens_to_vectors_udf(\"lemmatized_tokens\"))\n",
    "            .drop(\"temporary_vec\"))\n",
    "\n",
    "df_train = transform_dataset(df_train)\n",
    "df_valid = transform_dataset(df_valid)\n",
    "df_test = transform_dataset(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41720b",
   "metadata": {},
   "source": [
    "- Next, the padding will be applied so the vectorized tokens can have the same length for the model requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4e996546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to pad (or truncate) sequences\n",
    "def pad_sequence(tokens, length=187):\n",
    "    return tokens[:length] + [0] * (length - len(tokens))\n",
    "\n",
    "# Convert the Python function to a PySpark UDF\n",
    "pad_sequence_udf = udf(lambda x: pad_sequence(x), ArrayType(IntegerType(), containsNull=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f6f46c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding to the training dataset\n",
    "df_train = df_train.withColumn(\"padded_sequence\", pad_sequence_udf(col(\"Review_Vec_Sequence\")))\n",
    "\n",
    "# Apply padding to the validation dataset\n",
    "df_valid = df_valid.withColumn(\"padded_sequence\", pad_sequence_udf(col(\"Review_Vec_Sequence\")))\n",
    "\n",
    "# Apply padding to the test dataset\n",
    "df_test = df_test.withColumn(\"padded_sequence\", pad_sequence_udf(col(\"Review_Vec_Sequence\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c20796f",
   "metadata": {},
   "source": [
    "Neural Networks, and LSTM is not different required a certain 3-D numpy array shape. Howevever, in the experiment the final step between transforming the data to the right shape did not work and a few approaches were attempted to overcome the issue, but still \"error\" persisted.\n",
    "Issues:\n",
    "Memory problems: The error about memory issues were faced multiple times at different stages of the notebook. At first the issue were appearing at the preprocessing stage where the entire dataset was manipulated and after a few adjustments like reducing by half made the work flows until the transformation from spark dataframe to a numpy. \n",
    "However during the last stage many changes and attempts to overcome the problem, which were: \n",
    "- Adjusted the dataset ratio using 100%, 50%, 20%, 10%, 1%;\n",
    "- Revised all the steps to check and correct if any dataframes were being created without purpose, impacting in the memory effiency;\n",
    "- Also, in the vectorizing stage, different vector sizes were tried, 200, 150, 100, 50. \n",
    "- Tried different paths, like changing to pandas and after to numpy format.\n",
    "\n",
    "After all the attempts, which were many, it was resulting in the same issue, memory. Therefore, due to time constrains the application of the LSTM model was not able to tweaked further to fix the problem. Eventhough the model was not applied, the code was created below, to fit the LSTM model using the training set, with the necessary features for the embedding layer. \n",
    "\n",
    "The activation function chosen was the softmax, given the objective which is multi-classification, is the one that provides the probility of the right sentiment. Also, because sentiment analysis tend to overfit the spatial dropout were added to mitigate is risk. The optimizer selected was the adam, mainly because is the \"go to\" before experiment with other optimizers to check their effiency. And last but not least, following the same rationale as the optimizer, the loss function was the categorical crossentropy, which is the most adequate for the objective.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a31e1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def vector_to_list(v):\n",
    " #   return v.toArray().tolist()\n",
    "\n",
    "#vector_to_list_udf = udf(vector_to_list, ArrayType(FloatType()))\n",
    "\n",
    "#df_train = df_train.withColumn(\"padded_sequence\", vector_to_list_udf(col(\"padded_sequence\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "81b72408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Rating: integer (nullable = true)\n",
      " |-- Review: string (nullable = true)\n",
      " |-- label: string (nullable = false)\n",
      " |-- Review_lower: string (nullable = true)\n",
      " |-- word_count: integer (nullable = false)\n",
      " |-- Review_no_punct: string (nullable = true)\n",
      " |-- Review_no_punct2: string (nullable = true)\n",
      " |-- tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filt_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- filtered_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- t_no_stopwords: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- lemmatized_tokens: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- label_index: double (nullable = false)\n",
      " |-- label_encoded: vector (nullable = true)\n",
      " |-- sequence_length: integer (nullable = true)\n",
      " |-- Review_Vec_Sequence: array (nullable = true)\n",
      " |    |-- element: vector (containsNull = true)\n",
      " |-- padded_sequence: array (nullable = true)\n",
      " |    |-- element: integer (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb5052e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-11 20:04:37,037 ERROR python.PythonRunner: Python worker exited unexpectedly (crashed)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/worker.py\", line 599, in main\n",
      "    eval_type = read_int(infile)\n",
      "  File \"/usr/local/spark/python/lib/pyspark.zip/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:556)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:762)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:744)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:509)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2264)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1491)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.ml.linalg.DenseVector)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n",
      "2023-10-11 20:04:37,082 ERROR python.PythonRunner: This may have been caused by a prior exception:\n",
      "net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.ml.linalg.DenseVector)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n",
      "2023-10-11 20:04:37,116 ERROR executor.Executor: Exception in task 0.0 in stage 102.0 (TID 100)\n",
      "net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.ml.linalg.DenseVector)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n",
      "2023-10-11 20:04:37,209 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 102.0 (TID 100) (10.0.2.15 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.ml.linalg.DenseVector)\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n",
      "\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n",
      "\n",
      "2023-10-11 20:04:37,213 ERROR scheduler.TaskSetManager: Task 0 in stage 102.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 102.0 failed 1 times, most recent failure: Lost task 0.0 in stage 102.0 (TID 100) (10.0.2.15 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.ml.linalg.DenseVector)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.ml.linalg.DenseVector)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4904/649298889.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Takes \"padded_sequence\" column as a list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"padded_sequence\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatMap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Convert the list to a NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mX_train_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_sequence_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    948\u001b[0m         \"\"\"\n\u001b[1;32m    949\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 102.0 failed 1 times, most recent failure: Lost task 0.0 in stage 102.0 (TID 100) (10.0.2.15 executor driver): net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.ml.linalg.DenseVector)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2450)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2399)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2398)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2398)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1156)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1156)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2638)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2580)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2569)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:938)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2224)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2245)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2264)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2289)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for pyspark.ml.linalg.DenseVector)\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:773)\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:213)\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:123)\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:136)\n\tat org.apache.spark.sql.execution.python.BatchEvalPythonExec.$anonfun$evaluate$6(BatchEvalPythonExec.scala:94)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:86)\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:80)\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:307)\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeIteratorToStream(PythonRunner.scala:729)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:435)\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:2048)\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:270)\n"
     ]
    }
   ],
   "source": [
    "# Takes \"padded_sequence\" column as a list\n",
    "X_train = df_train.select(\"padded_sequence\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# Convert the list to a NumPy array\n",
    "X_train_np = np.array(padded_sequence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae347b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train =  np.array(df_train.select(\"padded_sequence\").collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72347f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the \"padded_sequence\" column as a list of lists.\n",
    "padded_sequence_list = df_train.select(\"padded_sequence\").rdd.flatMap(lambda x: x[0]).collect()\n",
    "\n",
    "# Converts the list to a NumPy array.\n",
    "X_train_np = np.array(padded_sequence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3375816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6263a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_pd = df_train.select(\"padded_sequence\", \"label_encoded\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb735e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = df_train[\"padded_sequence\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6528b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_train = df_train[\"label_encoded\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beffe630",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "\n",
    "# Defining the the model.\n",
    "model_lstm = Sequential()\n",
    "\n",
    "# Add an Embedding layer.\n",
    "embedding_dim = 50  # Setting the same embedding dimension as the Word2Vec model.\n",
    "vocab_size = 20000   # Vocabulary size, in this was set to be 15000 (max_vocab).\n",
    "max_length = 195    # Max sequence length.\n",
    "\n",
    "model_lstm.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "\n",
    "# Adding SpatialDropout1D layer to prevent overfitting.\n",
    "model_lstm.add(SpatialDropout1D(0.2))  # dropout 20%\n",
    "\n",
    "# Add an LSTM layer\n",
    "lstm_units = 128  # Hidden layer with 128 neurons.\n",
    "model_lstm.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Three output layer (positive, negative and neutral)\n",
    "num_classes = 3  # Positive, negative and neutrals.\n",
    "model_lstm.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "model_lstm.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173cea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the number of epochs and batch size\n",
    "epochs = 10  # Epochs, how many times it will run throught the data..\n",
    "batch_size = 32  \n",
    "\n",
    "# Train the model on train set.\n",
    "fitted_lstm = model_lstm.fit(\n",
    "    X_train,  # Your preprocessed and padded training data\n",
    "    y_train,        # One-hot encoded training labels\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=(X_valid, y_valid),  # Validation data\n",
    "    verbose=2)\n",
    "\n",
    "# Evaluate the model on your test data\n",
    "test_loss, test_accuracy = model_lstm.evaluate(X_test, y_test, verbose=2)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da64756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model.\n",
    "model_lstm_2 = Sequential()\n",
    "\n",
    "# Add an Embedding layer.\n",
    "model_lstm_2.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length))\n",
    "model_lstm_2.add(SpatialDropout1D(0.2))\n",
    "\n",
    "# Add the first LSTM layer.\n",
    "model_lstm_2.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "\n",
    "# Add more LSTM layers.\n",
    "model_lstm_2.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2, return_sequences=True))\n",
    "model_lstm_2.add(LSTM(units=lstm_units, dropout=0.2, recurrent_dropout=0.2))\n",
    "\n",
    "# Adds a dense hidden layer.\n",
    "model_lstm_2.add(Dense(128, activation='relu'))\n",
    "\n",
    "# Adds an output layer.\n",
    "model_lstm_2.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compiles the model.\n",
    "model_lstm_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Prints the model summary.\n",
    "model_lstm_2.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
